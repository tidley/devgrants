# Open Grant Proposal: `Decentralised Automotive Development Test Data`

**Distributed Automotive Development:**

**Proposal Category:** `app-dev`

**Proposer:** `tidley`

**Do you agree to open source all work you do on behalf of this RFP and dual-license under MIT and APACHE2 licenses?:** Yes

# Project Description

_Q_

Please describe exactly what you are planning to build. Make sure to include the following:

- Start with the need or problem you are trying to solve with this project.

_A_

###### Read https://www.dataversity.net/brief-history-analytics/

1. where data come from
1. what data are used for
1. requirements
1. issues

Data storage is an issue at automotive engineering facilities where large volumes of test data are generated and must be made available for analysis. The issue can be broken up into:

1. Storage - The physical storage requirements grow exponentially as tests become increasingly sophisticated to match legislative requirements.
1. Indexing - One test can generate data files from multiple test instruments, each covering varying time spans and sample rates, all needing to be linked to the same test.
1. Retrieval - Accessing large datasets can put a strain on IT network infrastructure.
1. Processing - Analysis of data requires having confidence in the data source to ensure effective processing.

Traditional client-server architecture is relied upon for the majority of data handling although other methods can sometimes be used to access data. The pitfalls of each of these are outlined:

**1. Relational databases (SQL, Oracle) running on server-grade hardware with configurable client software**

- Artificial restrictions set by the suppliers (number of active connections, feature sets, CPU cores).
- Physical restrictions created by the infrastructure (network bandwidth, data media read/write speeds).
- Multiple single points of failure on the network and host machine (connection issues, power loss, corrupt media).

**2. Networked machines, both where the data originates and employee devices**

- Data accessed directly from the source can cause excessive wear on storage medium that weren't designed for high numbers of read/write operations.
- Over-use of the local CPU to serve data can cause performance issues if running test software.
- Dispersed, un-checked copies of data increases data storage waste and reduces data integrity.

**3. Network-share drives with access configured using domain controllers and active directories often running on VM's (Windows Server)**

- Data files can easily be miss-placed and duplicated by employees leading to confusion as to the integrity of data.
- Folder access permissions are granted on a per-user and user-group basis which can become unwieldy with multiple data locations and transitioning employee roles.

**4. Removable media such as USB hard-drives and pen-drives**

- Data are much more likely to be lost when carried on a person.
- Data are more corruptible since removable media storage hardware are designed with fewer read/write cycles than even consumer-grade physical storage media such as SSD's and hard drives.

**5. E-mails**

- Unnecessary use of email servers to store test data.
- Data are only accessible by employees who received the e-mail.
- Unless data are e-mailed it is not necessarily accessible by project engineers.

generate masses of measurement data from tests of automotive powertrains to model the performance. Specialised test instrumentation are capable of sampling at megahertz-frequencies and can generate gigabytes of data within a few seconds. Data from instruments involved in each test must be reliably stored and made available for analysis. Statistical analysis depends on the integrity of data to generate optimal solutions. Missing or erroneous data will reduce model confidence, leading to less effective results.

Centralised servers are used to process most data at test facilities with access policies managed by a series of domain controllers and active directories. Storage is centralised on servers with network shares and client-software used for access. Data may also be handled by several intermediate computers, where data are duplicated and stored on laptops and workstations.

Handling gigabytes of test data is a computationally expensive task that puts strain on the network infrastructure, including network switches, routers and physical storage media.

New files must be made quickly available to engineers and automated scripts since automotive development is time-sensitive and to recorded in a database to allow access by engineers and automated scripts. The nature of since the tests carried out are expensive to run and a second chance might not be possible.

Data management systems in automotive engineering test facilities process gigabytes of data from multiple sources, first storing and then making the data available to development engineers. Data are generated by multiple devices and must be aggregated for ease of access.

Data management in automotive engineering is inherently inefficient due to a reliance on centralised servers. The task of retrieving data is physically limited by network switches, hard-drives, processing power and database coverage using a centralised architecture. Furthermore, data retrieval can be complicated by having to navigate multiple servers with seperate credentials.

An example of the most common forms of data storage and their faults are listed below:

These issues can be remedied using a decentralised approach built around IPFS. Clusters of nodes in each discrete network location such as a test cell or corridor. The outline of the process would be:

1. Newly generated data are replicated to an IPFS cluster for local access and to Filecoin for archiving.
1. Each new file will exist on the cluster for a pre-determined time after the last time accessed.
1. Records of each file will be stored on OrbitDB to allow indexing, each entry will contain time of creation, origin, project, test name, location(s), duplicates, is-archived, last accessed, expiry date, failed copies, etc. for each file.
1. Each device able to run the client software on the network will be assigned an IPFS ID to allow it to write to and read the OrbitDB database and have it's activities traced throughout it's lifecycle.
1. Locations of devices is updated autonomously using the currently connected network port.
1. Instrument locations updated in real-time (address, instrument type, instrument ID, serial #, ...)
1. Data are assigned to active projects and so can be easily discovered
1. (data sources (instrument ID, times, ...))

_Q_

- Describe why your solution is going to adequately solve this problem.

_A_

The benefits of this approach:

1. Peer-to-peer communication would reduced network bottlenecks since computers would be able to communicate directly, bypassing central servers and being able to take the path of least resistance.
1.

Using an IPFS cluster will distribute the computational load and reduce the time taken for engineers to access data. Unused computer capacity (CPU clock cycles, hard-drive space, network connectivity... ) can be borrowed to perform processing, storage and distribution of data.

A GUI front-end on a suitable database (OrbitDB) will allow engineers to quickly search for and find their data; the IPFS cluster can then efficiently deliver data on request.

Excessive duplication of data is avoided with 3 copies pinned at any one time.

Once a set time is elapsed (e.g. 1 year after last accessed) a file is marked as being able to be removed from the IPFS network. Files are copied to long-term storage (Filecoin) the moment they're generated to preserve the original data.

## Value

_Q_

Please describe in more detail why this proposal is valuable for the Filecoin ecosystem. Answer the following questions:

- What are the benefits to getting this right?

_A_

A successful implementation at one facility could be readily mimicked at other facilities, which have similar requirements.

The requirements are highly demanding in terms of data storage, access and retrieval.

- Data generated by automotive engineering powertrain facilities is high-volume and high-value.
- Concepts such as 'big-data' is driving the industry to push for access to an increasing volume of digital information.
- The automotive engineering industry is highly compatible with the reductions in development costs made possible by data-driven processes such as CAD and DoE.
- Computer-controlled measurement equipment allows improvements in techniques according to Moore's law.

_Q_

- What are the risks if you don't get it right?

_A_

- Loss of confidence in distributed data solutions leading to a lack of potential for future projects.

_Q_

- What are the risks that will make executing on this project difficult?

_A_

- Non-standard operating systems, computer hardware and networks.
- Operational facilities will want to avoid interruptions to producing and processing data whilst migrating to a new system.
- Projects that have already started may find it difficult to transition to a novel network architecture.

This section should be 1-3 paragraphs long.

## Deliverables

_Q_

Please describe in details what your final deliverable for this project will be. Include a specification of the project and what functionality the software will deliver when it is finished.

_A - Specification_

1. Large data files (recorded at MHz frequency generating ~10GB in 1.5 minutes, or ~111MB/s\*) every 2 minutes must be able to be processed by the network. Processing includes:
   - Registered on OrbitDB.
   - Having 3 duplicates locally plus on Filecoin.
2. Registered users having access to new files.
   - Show available files with metadata.
3. User administration and authentication.
4. Content delivery system to enable effective data aggregation.

_A - Functionality_

1. New data files on specific machines are detected and replicated to the local IPFS cluster as well as Filecoin.
2. OrbitDB updated with new file information.
3. Intuitive, permissioned, data-access portal for users.
   - OrbitDB with information on each file (time, origin, # copies, locations, etc.)
4. Access right management controlled by a private key.
   - Read from an IPNS address.

## Development Roadmap

Please break up your development work into a clear set of milestones. This section needs to be very detailed (will vary on the project, but aim for around 2 pages for this section).

1. Data from several sources are duplicated to a dedicated IPFS cluster to isolate performance when processing high-volumes of data.
   - Determine e.g. if data read values should be limited when there's an active test to reduce system loading.
2.

3. Executable files to setup:

   _Storage nodes_

   1. Install required software to communicate on IPFS.
      - Detects hard drive read/write speed, network speed etc. to generate system score.
   2. Menu for configuring things such as:
      - Database directory
      - % HDD to leave free
      - Bootstrap nodes
   3. Identifies and registers self on network.

   _Admin portal_

   1. Provides relevant admin features to monitor and configure the network

   _Data origins_

   1. Installs required software to communicate on IPFS.
   2. Provides custom instrument configuration menu to define data source including:
      - Interface (ethernet, serial)
      - Internal / external (mirror or read)
      - Timings (baud rate etc.)
      - Data directory (if to monitor folder for new files)
   3. Identifies and registers self on network.

## 2. Executable to run on each oracle which:

- Provides relevant admin features to monitor and configure the network.

## 3. Executable to run on each data generator which:

- Installs required software to communicate on IPFS.
- Provides custom instrument configuration menu to define data source including:
  - Interface (ethernet, serial)
  - Internal / external (mirror or read)
  - Timings (baud rate etc.)
  - Data directory (if to monitor folder for new files)
  -
- Identifies and registers self on network.

For each milestone, please describe:

_Q_

- The software functionality that we can expect after the completion of each milestone. This should be detailed enough that it can be used to ensure that the software meets the specification you outlined in the Deliverables.

_A_

-

_Q_

- How many people will be working on each milestone and their roles

_A_

-

_Q_

- The amount of funding required for each milestone

_A_

-

_Q_

- How much time this milestone will take to achieve (using real dates)

_A_

-

## Total Budget Requested

Sum up the total requested budget across all milestones, and include that figure here. Also, please include a budget breakdown to specify how you are planning to spend these funds.

## Maintenance and Upgrade Plans

Specify your team's long-term plans to maintain this software and upgrade it over time.

# Team

## Team Members

- Team Member 1
- Team Member 2
- Team Member 3
- ...

## Team Member LinkedIn Profiles

- Team Member 1 LinkedIn profile
- Team Member 2 LinkedIn profile
- Team Member 3 LinkedIn profile
- ...

## Team Website

Please link to your team's website here (make sure it's `https`)

## Relevant Experience

_Q_

Please describe (in words) your team's relevant experience, and why you think you are the right team to build this project. You can cite your team's prior experience in similar domains, doing similar dev work, individual team members' backgrounds, etc.

_A_

## Team code repositories

Please provide links to your team's prior code repos for similar or related projects.

# Additional Information

Please include any additional information that you think would be useful in helping us to evaluate your proposal.
